{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f15fByizDBG5",
        "outputId": "d5a68cc1-ed80-49c0-fc76-23faf705ef1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HFnLXNuCE4Q",
        "outputId": "60e5e10a-a08e-4c1d-ebf1-4ebbcc55b0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Xenova/quickdraw-small\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U-TQCVlC6JV",
        "outputId": "b8b32fcf-c2fc-4b52-fd67-670a0d000956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 4500000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 250000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 250000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = (dataset['train'][1]['image'])"
      ],
      "metadata": {
        "id": "jR3BPDcPDEbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "siM1AN9UDGx3",
        "outputId": "38be62e2-6fcf-42c1-c42a-28e042bea65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABi0lEQVR4nH3SWyhDcRwH8O/ZFiGNLCSWQsQDtge1B5RLHqZETQgP2gNRbrkl3uRtUkqyV28jD9KiFmOay4SMF02Uy4vLFhvbOT8Ps7Od4/J9+p4+5/f79z8d4J8wwsfiSs5zYv/91TkiIrLX/mbDNB0fq+g4pfmYH1bGGgEA0snAQbQYN66jvpuWmxBZKg3w3eQCAEkYC7HDd0tmshDfED5IBq8QH5HC97RXz5/IVDmD83zc7+mhqi3Sia9isYXavlMixjG/PFhqqCW4PQJVR4O+c0e6TtnmymPFkzj0cpcBN7HUG7pQOFNqIAq3skRfiXiMMZDjk8rXz6pp2acUmtRIhpc10uip4GbVvSgwpZkbGuH0pFb4x6c/ZgJ5ESu73M+t0XcrWqrD5nE+9T0t8ZazRRdp0JOmga7QSdkHu+NswrfJPWStgMRpRTPNIpXt7+Y0pfzSplwA9VSLHlIB56YY157or7Q5GZhvASw8oJFGBSa7bwJmDADaKYPZPvvx+YKJ00uRlPUHhvMFD++PvSgw+FgAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Edw_nlNRDIVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset['train']"
      ],
      "metadata": {
        "id": "2xweJUmKDLNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]['image'].resize((64,64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "giTQqQA6I8io",
        "outputId": "51795e80-5322-439f-8844-e6007c176f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x64>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAADn0lEQVR4nO2VXW8bRRSG3/na2fVXYjvBUhxSKqqgViCoBFzABZT/yy/ghnuEgKqiRVXdxjRxQh3byTrendmZOVzYTh3byQXi0u9+SLtz5pl5d86cBTbaaKON/icxMKG05ncFrL4jwI5zAgAJ0WwfPqiBsdVAAIwJKYAlCpF7/fMzMwM0Dr//tmECXw/gMpKMlmeg8GuvMwdUWvsH+iy7xQHjknPyLrwfngm1U2m3trMcgARParV40jn1oLVzAAODzd31c+BR5ZMv9HZjZAiQgFBJdPzLq7ACoOmNALjCL8wgqouHSbWipxYQAsfbn56tDk2zk5Yb5G7jiU5iNQOAOMZveuu/wXqlp6Ycl+QcAAKTt8UyqWTEr65uvMxNEHGiGM0AAFc3u3ElhIhiJZVSWmnePxosrAO880yXri2s0c7Dj/a3olhKIYTicG//ePGy/76ZjKOoFN0BqD/+5qt9sFkeE3U/SM4XACis13cCGIt3JM6v8iL4XOy1t3aberHdZVbdaSE7640qnd+O+xNvh/q7Jwzqxn5zk1yWInY74Oqoc4n07+evU19cxPtflplYBohrC4wx0FKyXD6vDhCuei9ShEKNTIluRrgrI8tzCxQIfKkg2MHxmFCk/xgAWeaXB/CZFUkEABwUQgBfseJ9gBAMWK4Fc4BM1AxgMwtVWo7iFJhUCgDEaqlwk1xNNxNHSPtjJPXKcogLK8YWLKZ5qZEQAA4/6g1Ddb8d3wxhwYOvr1IA7DCNmxUBQCKkvXdZ42v56tx7773z3ntfFDVFWFg6Cm4RMOieuMr21siTRJj0joetHx5132VFlpssN3luLsftEi2YJwphETB60500m60sIwnyl53fH9Xu1y6Ms9Zaa/PcXI7vtSQX8/5M6OpunXHOGeecc7Z90NRqe+/cBAnAPfux83krlKpSCikZgne2KLdFNE9fJvTW3qdaxHp6aF1tHOzx+sHJyEkA1M2G2b1Yx3GSxFIpzhhjZM3ZcDKdd5Fj6+Bxi5d1nMRxEmudxKUwzqOITUva5DSM6kpJNb2UiqJICPPyz44FAIzP23utD8ckQCAQkbsYBRo87Qz8dDP57KjLMEs5JqM4SeKkSr2TsQUAGnZ2m+UyvDOTPDPZJM8yY4r+07/6bl2aAmBRFCVI06kD/uCzw4+rHIUtcmusMcYaa4v0pDcJt/1LGGMMIcy2kI7LNRlcUSAQEQUiIqJQuLBS8TfaaKON/rv+BSUmvgjV/X0/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = train_data.with_format(\"torch\")"
      ],
      "metadata": {
        "id": "--KU1uf_DP6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['aircraft carrier',\n",
        " 'airplane',\n",
        " 'alarm clock',\n",
        " 'ambulance',\n",
        " 'angel',\n",
        " 'animal migration',\n",
        " 'ant',\n",
        " 'anvil',\n",
        " 'apple',\n",
        " 'arm',\n",
        " 'asparagus',\n",
        " 'axe',\n",
        " 'backpack',\n",
        " 'banana',\n",
        " 'bandage',\n",
        " 'barn',\n",
        " 'baseball bat',\n",
        " 'baseball',\n",
        " 'basket',\n",
        " 'basketball',\n",
        " 'bat',\n",
        " 'bathtub',\n",
        " 'beach',\n",
        " 'bear',\n",
        " 'beard',\n",
        " 'bed',\n",
        " 'bee',\n",
        " 'belt',\n",
        " 'bench',\n",
        " 'bicycle',\n",
        " 'binoculars',\n",
        " 'bird',\n",
        " 'birthday cake',\n",
        " 'blackberry',\n",
        " 'blueberry',\n",
        " 'book',\n",
        " 'boomerang',\n",
        " 'bottlecap',\n",
        " 'bowtie',\n",
        " 'bracelet',\n",
        " 'brain',\n",
        " 'bread',\n",
        " 'bridge',\n",
        " 'broccoli',\n",
        " 'broom',\n",
        " 'bucket',\n",
        " 'bulldozer',\n",
        " 'bus',\n",
        " 'bush',\n",
        " 'butterfly',\n",
        " 'cactus',\n",
        " 'cake',\n",
        " 'calculator',\n",
        " 'calendar',\n",
        " 'camel',\n",
        " 'camera',\n",
        " 'camouflage',\n",
        " 'campfire',\n",
        " 'candle',\n",
        " 'cannon',\n",
        " 'canoe',\n",
        " 'car',\n",
        " 'carrot',\n",
        " 'castle',\n",
        " 'cat',\n",
        " 'ceiling fan',\n",
        " 'cell phone',\n",
        " 'cello',\n",
        " 'chair',\n",
        " 'chandelier',\n",
        " 'church',\n",
        " 'circle',\n",
        " 'clarinet',\n",
        " 'clock',\n",
        " 'cloud',\n",
        " 'coffee cup',\n",
        " 'compass',\n",
        " 'computer',\n",
        " 'cookie',\n",
        " 'cooler',\n",
        " 'couch',\n",
        " 'cow',\n",
        " 'crab',\n",
        " 'crayon',\n",
        " 'crocodile',\n",
        " 'crown',\n",
        " 'cruise ship',\n",
        " 'cup',\n",
        " 'diamond',\n",
        " 'dishwasher',\n",
        " 'diving board',\n",
        " 'dog',\n",
        " 'dolphin',\n",
        " 'donut',\n",
        " 'door',\n",
        " 'dragon',\n",
        " 'dresser',\n",
        " 'drill',\n",
        " 'drums',\n",
        " 'duck',\n",
        " 'dumbbell',\n",
        " 'ear',\n",
        " 'elbow',\n",
        " 'elephant',\n",
        " 'envelope',\n",
        " 'eraser',\n",
        " 'eye',\n",
        " 'eyeglasses',\n",
        " 'face',\n",
        " 'fan',\n",
        " 'feather',\n",
        " 'fence',\n",
        " 'finger',\n",
        " 'fire hydrant',\n",
        " 'fireplace',\n",
        " 'firetruck',\n",
        " 'fish',\n",
        " 'flamingo',\n",
        " 'flashlight',\n",
        " 'flip flops',\n",
        " 'floor lamp',\n",
        " 'flower',\n",
        " 'flying saucer',\n",
        " 'foot',\n",
        " 'fork',\n",
        " 'frog',\n",
        " 'frying pan',\n",
        " 'garden hose',\n",
        " 'garden',\n",
        " 'giraffe',\n",
        " 'goatee',\n",
        " 'golf club',\n",
        " 'grapes',\n",
        " 'grass',\n",
        " 'guitar',\n",
        " 'hamburger',\n",
        " 'hammer',\n",
        " 'hand',\n",
        " 'harp',\n",
        " 'hat',\n",
        " 'headphones',\n",
        " 'hedgehog',\n",
        " 'helicopter',\n",
        " 'helmet',\n",
        " 'hexagon',\n",
        " 'hockey puck',\n",
        " 'hockey stick',\n",
        " 'horse',\n",
        " 'hospital',\n",
        " 'hot air balloon',\n",
        " 'hot dog',\n",
        " 'hot tub',\n",
        " 'hourglass',\n",
        " 'house plant',\n",
        " 'house',\n",
        " 'hurricane',\n",
        " 'ice cream',\n",
        " 'jacket',\n",
        " 'jail',\n",
        " 'kangaroo',\n",
        " 'key',\n",
        " 'keyboard',\n",
        " 'knee',\n",
        " 'knife',\n",
        " 'ladder',\n",
        " 'lantern',\n",
        " 'laptop',\n",
        " 'leaf',\n",
        " 'leg',\n",
        " 'light bulb',\n",
        " 'lighter',\n",
        " 'lighthouse',\n",
        " 'lightning',\n",
        " 'line',\n",
        " 'lion',\n",
        " 'lipstick',\n",
        " 'lobster',\n",
        " 'lollipop',\n",
        " 'mailbox',\n",
        " 'map',\n",
        " 'marker',\n",
        " 'matches',\n",
        " 'megaphone',\n",
        " 'mermaid',\n",
        " 'microphone',\n",
        " 'microwave',\n",
        " 'monkey',\n",
        " 'moon',\n",
        " 'mosquito',\n",
        " 'motorbike',\n",
        " 'mountain',\n",
        " 'mouse',\n",
        " 'moustache',\n",
        " 'mouth',\n",
        " 'mug',\n",
        " 'mushroom',\n",
        " 'nail',\n",
        " 'necklace',\n",
        " 'nose',\n",
        " 'ocean',\n",
        " 'octagon',\n",
        " 'octopus',\n",
        " 'onion',\n",
        " 'oven',\n",
        " 'owl',\n",
        " 'paint can',\n",
        " 'paintbrush',\n",
        " 'palm tree',\n",
        " 'panda',\n",
        " 'pants',\n",
        " 'paper clip',\n",
        " 'parachute',\n",
        " 'parrot',\n",
        " 'passport',\n",
        " 'peanut',\n",
        " 'pear',\n",
        " 'peas',\n",
        " 'pencil',\n",
        " 'penguin',\n",
        " 'piano',\n",
        " 'pickup truck',\n",
        " 'picture frame',\n",
        " 'pig',\n",
        " 'pillow',\n",
        " 'pineapple',\n",
        " 'pizza',\n",
        " 'pliers',\n",
        " 'police car',\n",
        " 'pond',\n",
        " 'pool',\n",
        " 'popsicle',\n",
        " 'postcard',\n",
        " 'potato',\n",
        " 'power outlet',\n",
        " 'purse',\n",
        " 'rabbit',\n",
        " 'raccoon',\n",
        " 'radio',\n",
        " 'rain',\n",
        " 'rainbow',\n",
        " 'rake',\n",
        " 'remote control',\n",
        " 'rhinoceros',\n",
        " 'rifle',\n",
        " 'river',\n",
        " 'roller coaster',\n",
        " 'rollerskates',\n",
        " 'sailboat',\n",
        " 'sandwich',\n",
        " 'saw',\n",
        " 'saxophone',\n",
        " 'school bus',\n",
        " 'scissors',\n",
        " 'scorpion',\n",
        " 'screwdriver',\n",
        " 'sea turtle',\n",
        " 'see saw',\n",
        " 'shark',\n",
        " 'sheep',\n",
        " 'shoe',\n",
        " 'shorts',\n",
        " 'shovel',\n",
        " 'sink',\n",
        " 'skateboard',\n",
        " 'skull',\n",
        " 'skyscraper',\n",
        " 'sleeping bag',\n",
        " 'smiley face',\n",
        " 'snail',\n",
        " 'snake',\n",
        " 'snorkel',\n",
        " 'snowflake',\n",
        " 'snowman',\n",
        " 'soccer ball',\n",
        " 'sock',\n",
        " 'speedboat',\n",
        " 'spider',\n",
        " 'spoon',\n",
        " 'spreadsheet',\n",
        " 'square',\n",
        " 'squiggle',\n",
        " 'squirrel',\n",
        " 'stairs',\n",
        " 'star',\n",
        " 'steak',\n",
        " 'stereo',\n",
        " 'stethoscope',\n",
        " 'stitches',\n",
        " 'stop sign',\n",
        " 'stove',\n",
        " 'strawberry',\n",
        " 'streetlight',\n",
        " 'string bean',\n",
        " 'submarine',\n",
        " 'suitcase',\n",
        " 'sun',\n",
        " 'swan',\n",
        " 'sweater',\n",
        " 'swing set',\n",
        " 'sword',\n",
        " 'syringe',\n",
        " 't-shirt',\n",
        " 'table',\n",
        " 'teapot',\n",
        " 'teddy-bear',\n",
        " 'telephone',\n",
        " 'television',\n",
        " 'tennis racquet',\n",
        " 'tent',\n",
        " 'The Eiffel Tower',\n",
        " 'The Great Wall of China',\n",
        " 'The Mona Lisa',\n",
        " 'tiger',\n",
        " 'toaster',\n",
        " 'toe',\n",
        " 'toilet',\n",
        " 'tooth',\n",
        " 'toothbrush',\n",
        " 'toothpaste',\n",
        " 'tornado',\n",
        " 'tractor',\n",
        " 'traffic light',\n",
        " 'train',\n",
        " 'tree',\n",
        " 'triangle',\n",
        " 'trombone',\n",
        " 'truck',\n",
        " 'trumpet',\n",
        " 'umbrella',\n",
        " 'underwear',\n",
        " 'van',\n",
        " 'vase',\n",
        " 'violin',\n",
        " 'washing machine',\n",
        " 'watermelon',\n",
        " 'waterslide',\n",
        " 'whale',\n",
        " 'wheel',\n",
        " 'windmill',\n",
        " 'wine bottle',\n",
        " 'wine glass',\n",
        " 'wristwatch',\n",
        " 'yoga',\n",
        " 'zebra',\n",
        " 'zigzag']"
      ],
      "metadata": {
        "id": "kXM96YpbP3tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def transform(example_batch):\n",
        "    inputs = {}\n",
        "    inputs['image'] = torch.stack([torch.tensor(np.array(x.resize((64, 64)))).float().unsqueeze(0) for x in example_batch['image']])\n",
        "    inputs['label'] = torch.tensor(example_batch['label'])\n",
        "    return inputs\n",
        "\n",
        "prepared_ds = train_data.with_transform(transform)\n",
        "prepared_ds[0:2]['image']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "-Ckb7oRkIvBv",
        "outputId": "3f7918cc-0059-48c2-cc26-0201b15a3870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f7271abff4c2>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprepared_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprepared_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2810\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2794\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2795\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   2796\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRowFormat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f7271abff4c2>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(example_batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Patching(nn.Module):\n",
        "    \"\"\"\n",
        "    Patching module for extracting patches from input images.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, patch_size=16, embedding_dim=256):\n",
        "        super().__init__()\n",
        "        self.patch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, embedding_dim,\n",
        "                      kernel_size=(patch_size, patch_size),\n",
        "                      stride=(patch_size, patch_size)),\n",
        "            nn.Flatten(2, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Patching module.\n",
        "        \"\"\"\n",
        "        return self.patch(x).transpose(-2, -1)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    Head module for performing self-attention on input features.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embed, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.n_embed = n_embed\n",
        "        self.qkv = nn.Linear(n_embed, head_size * 3, bias=False)\n",
        "        self.attention_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the Head module.\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "        q, k, v = self.qkv(x).chunk(3, dim=2)\n",
        "        w = torch.bmm(k, q.transpose(-2, -1)) * (self.n_embed ** -0.5)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.unsqueeze(-1).float()\n",
        "            w = w * attention_mask\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = self.attention_dropout(w)\n",
        "        out = torch.bmm(w, v)\n",
        "        return out\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    \"\"\"\n",
        "    MultiHead module for combining multiple attention heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size, n_heads, n_embed,dropout):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(n_embed, head_size, dropout) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the MultiHead module.\n",
        "        \"\"\"\n",
        "        out = torch.cat([head(x, attention_mask) for head in self.heads], -1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    FeedForward module for applying a feed-forward neural network to input features.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embed, mlp_ratio, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, n_embed * mlp_ratio),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(n_embed * mlp_ratio, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the FeedForward module.\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Block module for a single transformer block.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embed, head_size, n_heads, dropout, mlp_ratio):\n",
        "        super().__init__()\n",
        "        self.multihead = MultiHead(head_size, n_heads, n_embed,dropout)\n",
        "        self.ffwd = FeedForward(n_embed, mlp_ratio, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the Block module.\n",
        "        \"\"\"\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.multihead(x, attention_mask)\n",
        "        x = self.ln2(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer model.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, patch_size=16, embedding_dim=256, head_size=32,\n",
        "                 n_heads=8, n_layers=6, dropout=0.4, mlp_ratio=2, block_size=64, num_classes=len(labels)):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = Patching(in_channels, patch_size, embedding_dim)\n",
        "        self.positional_embedding = nn.Embedding(block_size, embedding_dim)\n",
        "        self.blocks = nn.ModuleList([Block(embedding_dim, head_size, n_heads, dropout, mlp_ratio) for _ in range(n_layers)])\n",
        "        self.ln = nn.LayerNorm(embedding_dim)\n",
        "        self.sequence_pooling = nn.Linear(embedding_dim,1)\n",
        "        self.cl_head = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim * mlp_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim * mlp_ratio, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the ViT model.\n",
        "        \"\"\"\n",
        "        x = self.patch_embedding(x)\n",
        "        x = x + self.positional_embedding(torch.arange(x.shape[1], device=x.device))\n",
        "        for block in self.blocks:\n",
        "            x = block(x, attention_mask)\n",
        "        x = self.ln(x)\n",
        "        seq_pool = self.sequence_pooling(x).transpose(-2,-1) # B,1,N\n",
        "        seq_pool = torch.nn.functional.softmax(seq_pool,dim=2) # B,1,N\n",
        "        x = torch.bmm(seq_pool,x).squeeze(1) # B,D\n",
        "        x = self.cl_head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "goqfTypwDYsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "device = 'cuda'\n",
        "# model = ViT().to(device)\n",
        "model = torch.load('drive/MyDrive/model.pth')"
      ],
      "metadata": {
        "id": "M-7d_8NbQGb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
        "loader = DataLoader(prepared_ds, batch_size=10000, shuffle=True)\n",
        "for epoch in range(10):\n",
        "    loop = tqdm(loader,leave=False)\n",
        "    for batch_idx,data in enumerate(loop):\n",
        "        X = data['image']\n",
        "        y = data['label']\n",
        "        pred = model(X.type(torch.float32).to(device))\n",
        "        loss = criterion(pred,y.to(device))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        loop.set_postfix(loss=loss.item(),acc = torch.mean((y.to(device)==pred.argmax(axis=1)).type(torch.float32)).item())\n",
        "        loop.set_description(f\"Epoch : {epoch}\")\n",
        "      torch.save(model,'drive/MyDrive/model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1iMvqufOlQA",
        "outputId": "1a0e0209-2a20-4d17-8804-4c6f6b15f78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch : 3:  17%|█▋        | 78/450 [04:55<24:28,  3.95s/it, acc=0.602, loss=1.64]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: connect the drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQrIna2eOvGx",
        "outputId": "5270413e-217c-44a1-b3ce-7bfe54a0ce7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'drive/MyDrive/model.pth')"
      ],
      "metadata": {
        "id": "hJkWUnx5O_Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = dataset['test']\n",
        "test_data = test_data.with_transform(transform)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loader = DataLoader(test_data, batch_size=5000, shuffle=True)\n",
        "model.eval();\n",
        "with torch.no_grad():\n",
        "  loop = tqdm(loader,leave=False)\n",
        "  for batch_idx,data in enumerate(loop):\n",
        "      X = data['image']\n",
        "      y = data['label']\n",
        "      pred = model(X.type(torch.float32).to(device))\n",
        "      loss = criterion(pred,y.to(device))\n",
        "      loop.set_postfix(loss=loss.item(),acc = torch.mean((y.to(device)==pred.argmax(axis=1)).type(torch.float32)))\n",
        "\n",
        "\n",
        "model.train();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jChQOv-IQz0-",
        "outputId": "148bedb4-93d9-41b0-aa87-15d1aef4e3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZUOsPvgoqTu",
        "outputId": "e31114db-cd39-405e-dec9-4d2a3b57180a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6070, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "pEJhMr60o4kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TuaCs-dM1gQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}